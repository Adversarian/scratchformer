{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Progress Bar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will contain all of my experiments along the way to deepening my understanding of the Attention mechanism and Transformers. \n",
    "\n",
    "The idea is to later organise this into a clean library once I have a working version (or do this in parts as working pieces of the code are implemented in this notebook playground).\n",
    "\n",
    "I will be following Peter Bloem's blog on Transformers [here](https://peterbloem.nl/blog/transformers). I understand that this already comes with a [GitHub repository](https://github.com/pbloem/former) but I will try to implement my own version from scratch without looking up any of the code. Of course, not all resemblances will be coincidental but I'll do my best!   \n",
    "\n",
    "I will leave it intentionally unkempt and unedited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Boilerplate Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from einops import *\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: The Basic Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a $batch \\times {seq\\_len} \\times dim$ input matrix $X$, we calculate the scaled dot product with self.\n",
    "\n",
    "First we'll calculate the unnormalized weights $w'_{ij} = x_i^\\mathbf{T}x_j$. We could naively loop over all $(i, j)$ pairs for each batch of sequence vectors but there's an easier way.\n",
    "\n",
    "First, let's assume the simplest case when $batch = 1$. In this case we have a single sequence of vectors $X = <x_1, x_2, ..., x_n>$ each of size $dim$. If we multiply $X$ with its transpose $X^\\mathbf{T}$, we'll get exactly what we want: each row of $X$ will be multiplied by the each column $X^\\mathbf{T}$ which is, you guessed it, rows of $X$! Now adding the batch dimension doesn't change anything if we only permute the $seq\\_len$ and $dim$ dimensions.\n",
    "\n",
    "As it happens, torch already provides a facility for this exact situation in `torch.bmm` which stands for **Batched Matrix Multiply**. This can easily be implemented with `torch.matmul` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_prime(X):\n",
    "    return torch.bmm(X, X.transpose(1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to turn this into the scaled positive weights that sum to one, we'll have to apply a Softmax function to the **rows** of $W'$. Why? We know that in $W'$ in each batch, for each sequence, the weight of each element in the sequence is presented in the last dimension. \n",
    "\n",
    "Let's test out `w_prime` to see what the shape of the output is compared to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 64, 8])\n",
      "torch.Size([32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, dim = 32, 64, 8\n",
    "X = torch.randn(batch_size, seq_len, dim)\n",
    "print(X.shape)\n",
    "print(w_prime(X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense right? In each batch, for every sequence, every element in the sequence will have a weight against every other element in the sequence, itself included. So the shape of $W'$ will be $batch \\times seq\\_len \\times seq\\_len$ where the final dimension houses the unscaled weights. If we apply a softmax to this dimension we will get the scaled weights that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w_scaled(X):\n",
    "    unscaled_weights = w_prime(X)\n",
    "    return F.softmax(unscaled_weights, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.477756500244141\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(w_prime(X)[0,0,:].sum().item())\n",
    "print(w_scaled(X)[0,0,:].sum().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that's left is to weight each input sequence by its scaled self-attention weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_self_attention(X):\n",
    "    return torch.bmm(w_scaled(X), X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 64, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_self_attention(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here for each instance in a batch we have a $64 \\times 64$ weight matrix (where each row corresponds to the self-attention weights for the element in that position) being multiplied by a $64 \\times 8$ input matrix which ultimately results in a $32 \\times 64 \\times 8$ matrix where each sequence embedding is now weighted by self-attention scores.\n",
    "\n",
    "We'll also code the whole thing in `einsum` because we're cool like that and also because it **can be** potentially less spatially and temporally complex depending on the input size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I'm using different indices for axes that I know have the same dimensions beforehand\n",
    "# This is to guide the einsum operation towards the matrix multiplication result that I want\n",
    "# Also, using the same indice in the resultant matrix is illegal in einsum notation\n",
    "def cool_basic_self_attention(X):                                  \n",
    "    w_prime = einsum(X, X, \"b i j, b k j -> b i k\")\n",
    "    w_scaled = F.softmax(w_prime, dim=-1)                          \n",
    "    return einsum(w_scaled, X, \"b i j, b j k -> b i k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(basic_self_attention(X), cool_basic_self_attention(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Actual Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quoting the blogpost:\n",
    "\n",
    "> \"Every input vector $x_i$ is used in three different ways in he self attention operation:\n",
    "> - It is compared to every other vector to establish the weights for **its own output $y_i$**.\n",
    "> - It is compared to every other vector to establish the weigths for **the output of the j-th vector $y_j$**.\n",
    "> - It is used as part of the **weighted sum** to compute each output vector once the weights have been established.\"\n",
    "\n",
    "Now, instead of using the same input vector for all use cases, we calculate three new $dim \\times dim$ weights, namely $W_q, W_k, W_v$ which upon being multiplied by the input vector $x_i$ give us the values we'd use for each of the use cases above. I.e.:\n",
    "\n",
    "$$q_i = W_qx_i,  k_i = W_kx_i,  v_i=W_vx_i$$\n",
    "$$w'_{ij} = q_i^\\mathbf{T}k_j$$\n",
    "$$w_{ij} = softmax(w'_{ij})$$\n",
    "$$y_i =  \\sum_{j}w_{ij}v_j$$\n",
    "\n",
    "Another trick we'll use to make a real self-attention is to scale the dot product because the softmax function can be very sensitive to large input values.\n",
    "\n",
    "So we'll scale down $w'_{ij}$ by $\\sqrt{k}$. Why? As the blogpost explains, \"**Imagine a vector in $\\mathbf{R}^k$ with values all $c$. Its Euclidean length is $\\sqrt{k}c$. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors**\"\n",
    "\n",
    "$$w'_{ij} = \\frac{q_i^\\mathbf{T}k_j}{\\sqrt{k}} $$\n",
    "\n",
    "Lastly, we must account for the fact that an element in the sequence (in this particular case a word in a sentence) can mean different things to different neighbours. In a single self-attention operation, all this information gets summed together where we want them to be discriminating.\n",
    "\n",
    "This is achieved by combining several self-attention mechanisms for the same input vector, each with different weight matrices $W_q^r, W_k^r, W_v^r$ which are called *attention heads*.\n",
    "\n",
    "Each input $x_i$ will produce a different $y_i^r$ per attention head. We will concatenate all of these and pass them through a linear transformation to reduce the dimensionality back to $dim$. This whole mechanism is called the **multi-head self-attention**.\n",
    "\n",
    "There is a way to apply the MHA in an efficient way which is described in the blogpost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, dim, num_heads, causal_mask=False):\n",
    "        super().__init__()\n",
    "        assert (\n",
    "            dim % num_heads == 0\n",
    "        ), \"dim must be divisible by num_heads\"  # NOTE: this is part of the efficient implementation described in the blogpost\n",
    "        self.dim, self.num_heads, self.causal_mask = dim, num_heads, causal_mask\n",
    "        (\n",
    "            self.toq,\n",
    "            self.tok,\n",
    "            self.tov,\n",
    "        ) = (  # NOTE: bias=False because we're effectively using these as embeddings\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "            nn.Linear(dim, dim, bias=False),\n",
    "        )\n",
    "        self.projection_head = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, dim)\n",
    "        batch, seq_len, dim = x.shape\n",
    "        # q, k, v for ALL heads\n",
    "        q, k, v = self.toq(x), self.tok(x), self.tov(x)\n",
    "        # q, k, v now have the shape (batch, seq_len, dim)\n",
    "        # now we'll cut these into num_heads chunks for each attention head\n",
    "        # i.e, turn the last dimension from dim to num_heads * dim / num_heads\n",
    "        q, k, v = (\n",
    "            q.view(batch, seq_len, self.num_heads, dim // self.num_heads),\n",
    "            k.view(batch, seq_len, self.num_heads, dim // self.num_heads),\n",
    "            v.view(batch, seq_len, self.num_heads, dim // self.num_heads),\n",
    "        )\n",
    "        # we can \"fold\" the num_heads dimension onto the batch dimension\n",
    "        # because the scaled dot product part of the MHA is the same across\n",
    "        # both the batch and num_heads dimensions. In order to perform this\n",
    "        # folding, we first need to bring the num_heads dimension next to the batch dimension\n",
    "        # Side NOTE: view requires to be performed on a contiguous tensor\n",
    "        # Reshape does a similar operation without requiring a contiguous tensor\n",
    "        # but it returns a new tensor.\n",
    "        q, k, v = (\n",
    "            q.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch * self.num_heads, seq_len, dim // self.num_heads),\n",
    "            k.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch * self.num_heads, seq_len, dim // self.num_heads),\n",
    "            v.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch * self.num_heads, seq_len, dim // self.num_heads),\n",
    "        )\n",
    "        # q, k, v now have the shape (batch * num_heads, seq_len, dim / num_heads)\n",
    "        # now we'll compute the attention weights\n",
    "        scaled_dot_product = einsum(q, k, \"bh i j, bh k j -> bh i k\").div_(dim**0.5)\n",
    "        normalized_sdp = F.softmax(scaled_dot_product, dim=-1)\n",
    "\n",
    "        # now we'll apply the attention weights to the input to calculate the output for all heads\n",
    "        out = einsum(normalized_sdp, v, \"bh j k, bh k i -> bh j i\").view(\n",
    "            batch, self.num_heads, seq_len, dim // self.num_heads\n",
    "        )\n",
    "\n",
    "        # transpose again to put the num_heads dimension back where it belongs and reshape\n",
    "        out = out.transpose(1, 2).contiguous().view(batch, seq_len, dim)\n",
    "        # out now contains the concatenated attentions of all heads\n",
    "        # NOTE: each head is the size of dim / num_heads so that the concatenation ultimately adds up to dim again\n",
    "\n",
    "        # finally we'll run this through the final linear transformation to obtain the output\n",
    "        return self.projection_head(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 256])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MHA(256, 4)\n",
    "sample_input = torch.randn(1, 128, 256)\n",
    "mha(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Transformer Block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quote the blogpost's definition of a transformer, a transformer is:\n",
    "\n",
    "> \"Any architecture designed to process a connected set of units -- such as the tokens in a sequence or pixels in an image -- where theonly interaction between units is through self-attention.\"\n",
    "\n",
    "In order to build a transformer we first need to create its building blocks. Each transformer block applies in sequence:\n",
    "\n",
    "> \"a self attention layer, layer normalization, a feed forrward layer, and another layer normalization. Residual connections are added around both, before the normalization.\"\n",
    "\n",
    "NOTE: the layer norm is applied **over the embedding dimension only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, ff_dim_mult=4):\n",
    "        super().__init__()\n",
    "        assert ff_dim_mult > 1\n",
    "        self.attention = MHA(dim, num_heads)\n",
    "\n",
    "        self.layer_norm1 = nn.LayerNorm(dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim, ff_dim_mult * dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim_mult * dim, dim),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.attention(x)        # MHA\n",
    "        y = x + y                    # Residual Connection\n",
    "        y = self.layer_norm1(y)      # Layer Norm No. 1\n",
    "        out = self.feed_forward(y)   # Feed Forward\n",
    "        out = out + y                # Residual Connection\n",
    "        return self.layer_norm2(out) # Layer Norm No. 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_block = TransformerBlock(256, 4)\n",
    "sample_input = torch.randn(1, 128, 256)\n",
    "test_block(sample_input).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: A Classification Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the same example as the on in the blog post, we will first a simple sequence classification transformer with the IMDb sentiment classification dataset.\n",
    "\n",
    "First, however, we need to actually build our transformer.\n",
    "\n",
    "For a classification task, the most common way to build a transformer is to simply average out the output sequence of the transformer block and chuck it through a linear transformation down to 2 classes and then softmax to produce probabilities.\n",
    "\n",
    "We will be also using position and word embeddings.\n",
    "\n",
    "**Position embeddings:**\n",
    "These are used to make the transformer take the permutation of the sequence into account.\n",
    "These will be embedding layers for each position in a sequence. An alternative would be to use ...\n",
    "\n",
    "\n",
    "**Position encodings:**\n",
    "Work the same as embedding except the embeddings are not learned, we simply choose a function to map the positions to real valued vectors and let the network figure out how to interpret these encodings. (This is what's used in real transformers but it's complicated because the choice of the map function is a tricky hyperparameter to fiddle with)\n",
    "\n",
    "In favor of simplicity, we will use position embeddings for now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationTransformer(nn.Module):\n",
    "    def __init__(self, dim, num_heads, depth, seq_len, num_tokens, num_classes, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_tokens = num_tokens\n",
    "        self.word_emb = nn.Embedding(num_tokens, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.device = device\n",
    "\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(dim, num_heads) for i in range(depth)]\n",
    "        )\n",
    "\n",
    "        self.classification_head = nn.Linear(dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len = x.shape\n",
    "        word_embs = self.word_emb(x)\n",
    "        pos_embs = self.pos_emb(torch.arange(seq_len, device=self.device))[None, ...].expand(\n",
    "            batch, seq_len, self.dim\n",
    "        ) # we expand this so we can add it with word embeddings\n",
    "        x = word_embs + pos_embs\n",
    "        x = self.transformer_blocks(x).mean(dim=1)\n",
    "        return self.classification_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd50a837f5f34050b54e92eca3d1b307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "dataset = dataset.map(\n",
    "    lambda x: tokenizer(\n",
    "        x[\"text\"], return_tensors=\"np\", padding=True, max_length=512, truncation=True\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "dataset.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset[\"train\"], batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset[\"test\"], batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ClassificationTransformer(256, 8, 6, 512, 30522, 2).to(\"cuda\")\n",
    "optim = torch.optim.AdamW(transformer.parameters(), lr=1e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ClassificationTransformer                [64, 2]                   --\n",
       "├─Embedding: 1-1                         [64, 512, 256]            7,813,632\n",
       "├─Embedding: 1-2                         [512, 256]                131,072\n",
       "├─Sequential: 1-3                        [64, 512, 256]            --\n",
       "│    └─TransformerBlock: 2-1             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-1                     [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-2               [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-3              [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-4               [64, 512, 256]            512\n",
       "│    └─TransformerBlock: 2-2             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-5                     [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-6               [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-7              [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-8               [64, 512, 256]            512\n",
       "│    └─TransformerBlock: 2-3             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-9                     [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-10              [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-11             [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-12              [64, 512, 256]            512\n",
       "│    └─TransformerBlock: 2-4             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-13                    [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-14              [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-15             [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-16              [64, 512, 256]            512\n",
       "│    └─TransformerBlock: 2-5             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-17                    [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-18              [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-19             [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-20              [64, 512, 256]            512\n",
       "│    └─TransformerBlock: 2-6             [64, 512, 256]            --\n",
       "│    │    └─MHA: 3-21                    [64, 512, 256]            262,400\n",
       "│    │    └─LayerNorm: 3-22              [64, 512, 256]            512\n",
       "│    │    └─Sequential: 3-23             [64, 512, 256]            525,568\n",
       "│    │    └─LayerNorm: 3-24              [64, 512, 256]            512\n",
       "├─Linear: 1-4                            [64, 2]                   514\n",
       "==========================================================================================\n",
       "Total params: 12,679,170\n",
       "Trainable params: 12,679,170\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 870.19\n",
       "==========================================================================================\n",
       "Input size (MB): 0.26\n",
       "Forward/backward pass size (MB): 4497.34\n",
       "Params size (MB): 50.72\n",
       "Estimated Total Size (MB): 4548.32\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(transformer, input_data=torch.randint(0, 30522, (64, 512), device=\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5209: 100%|██████████| 391/391 [02:45<00:00,  2.36it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.6436\n",
      "\n",
      "Start of epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.418: 100%|██████████| 391/391 [02:46<00:00,  2.35it/s] \n",
      "100%|██████████| 391/391 [00:47<00:00,  8.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.76684\n",
      "\n",
      "Start of epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2668: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.812\n",
      "\n",
      "Start of epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3201: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.81096\n",
      "\n",
      "Start of epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2937: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.82988\n",
      "\n",
      "Start of epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3188: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.83608\n",
      "\n",
      "Start of epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2378: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8366\n",
      "\n",
      "Start of epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.1373: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.835\n",
      "\n",
      "Start of epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.2252: 100%|██████████| 391/391 [02:47<00:00,  2.34it/s]\n",
      "100%|██████████| 391/391 [00:47<00:00,  8.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.83332\n",
      "\n",
      "Start of epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.172: 100%|██████████| 391/391 [02:47<00:00,  2.33it/s] \n",
      "100%|██████████| 391/391 [00:47<00:00,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "for i in range(n_epochs):\n",
    "    print(f'\\nStart of epoch {i+1}')\n",
    "    transformer.train()\n",
    "    for batch in (pbar := tqdm(train_dataloader)):\n",
    "        optim.zero_grad()\n",
    "        input = batch[\"input_ids\"].to(\"cuda\")\n",
    "        label = batch[\"label\"].to(\"cuda\")\n",
    "        pred = transformer(input)\n",
    "        loss = criterion(pred, label)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pbar.set_description(f\"Loss: {round(loss.item(), 4)}\")\n",
    "    with torch.inference_mode():\n",
    "        transformer.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in tqdm(test_dataloader):\n",
    "            input = batch[\"input_ids\"].to(\"cuda\")\n",
    "            label = batch[\"label\"].to(\"cuda\")\n",
    "            pred = transformer(input)\n",
    "            correct += (pred.argmax(dim=1) == label).sum().item()\n",
    "            total += label.size(0)\n",
    "        print(f'Validation accuracy: {correct / total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), \"ctransformer.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: A Generative Transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
